# Copyright 2024 Swit Authors
# SPDX-License-Identifier: Apache-2.0
#
# Benchmark Workflow
# This workflow runs performance benchmarks and detects regressions
# Ref: Issue #831 - CI/CD ç®¡é“é›†æˆ

name: Performance Benchmarks

on:
  push:
    branches: [master]
    paths:
      - 'go.mod'
      - 'go.sum'
      - '**/*.go'
      - '.github/workflows/benchmark.yml'
  pull_request:
    branches: [master]
    paths:
      - 'go.mod'
      - 'go.sum'
      - '**/*.go'
      - '.github/workflows/benchmark.yml'
  schedule:
    # Run benchmarks weekly on Sunday at 3:00 AM UTC
    - cron: '0 3 * * 0'
  workflow_dispatch:
    inputs:
      benchmark_filter:
        description: 'Benchmark filter pattern (e.g., BenchmarkServer)'
        required: false
        default: ''
        type: string
      compare_base:
        description: 'Base branch/commit to compare against'
        required: false
        default: 'master'
        type: string

permissions:
  contents: read
  pull-requests: write

env:
  GO_VERSION: '1.24'
  BENCHMARK_COUNT: 5
  REGRESSION_THRESHOLD: 20  # Percentage threshold for regression detection

concurrency:
  group: benchmark-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # ===========================================================================
  # Run Benchmarks
  # ===========================================================================
  benchmark:
    name: Run Benchmarks
    runs-on: ubuntu-latest
    outputs:
      benchmark_results: ${{ steps.benchmark.outputs.results }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v6
        with:
          fetch-depth: 0

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }}
          check-latest: true
          cache: true
          cache-dependency-path: '**/go.sum'

      - name: Cache Go modules
        uses: actions/cache@v4
        with:
          path: ~/go/pkg/mod
          key: ${{ runner.os }}-go-${{ hashFiles('**/go.sum') }}
          restore-keys: |
            ${{ runner.os }}-go-

      - name: Restore generated code cache
        uses: actions/cache@v4
        with:
          path: |
            api/gen/
            docs/
            ~/.local/bin/
            ~/go/bin/
          key: ${{ runner.os }}-generated-${{ hashFiles('api/proto/**/*.proto', 'api/buf.gen.yaml', 'api/buf.yaml') }}
          restore-keys: |
            ${{ runner.os }}-generated-

      - name: Install dev tools
        run: make setup-dev

      - name: Generate proto code
        run: make proto

      - name: Install benchstat
        run: go install golang.org/x/perf/cmd/benchstat@latest

      - name: Run benchmarks
        id: benchmark
        run: |
          mkdir -p _output/benchmark
          
          FILTER="${{ github.event.inputs.benchmark_filter }}"
          if [ -z "$FILTER" ]; then
            FILTER="."
          fi
          
          echo "ðŸƒ Running benchmarks with filter: ${FILTER}"
          echo "ðŸ“Š Running ${BENCHMARK_COUNT} iterations for statistical significance"
          
          # Get all packages excluding generated code
          PACKAGES=$(go list ./... | grep -v -E '/(api/gen|docs/generated)/')
          
          # Run benchmarks
          go test -bench="${FILTER}" -benchmem -count=${BENCHMARK_COUNT} -timeout=30m \
            $PACKAGES 2>&1 | tee _output/benchmark/current.txt
          
          echo "âœ… Benchmarks completed"
          echo "results=_output/benchmark/current.txt" >> $GITHUB_OUTPUT

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: _output/benchmark/
          retention-days: 90

  # ===========================================================================
  # Compare with Base (for PRs)
  # ===========================================================================
  compare:
    name: Compare Benchmarks
    runs-on: ubuntu-latest
    needs: benchmark
    if: github.event_name == 'pull_request'
    steps:
      - name: Checkout code
        uses: actions/checkout@v6
        with:
          fetch-depth: 0

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }}
          check-latest: true
          cache: true
          cache-dependency-path: '**/go.sum'

      - name: Install benchstat
        run: go install golang.org/x/perf/cmd/benchstat@latest

      - name: Download current benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: _output/benchmark/

      - name: Checkout base branch
        run: |
          BASE_BRANCH="${{ github.event.inputs.compare_base || 'master' }}"
          git fetch origin ${BASE_BRANCH}
          git checkout origin/${BASE_BRANCH} -- . || true

      - name: Install dev tools for base
        run: make setup-dev || true

      - name: Generate proto code for base
        run: make proto || true

      - name: Run base benchmarks
        run: |
          mkdir -p _output/benchmark
          
          FILTER="${{ github.event.inputs.benchmark_filter }}"
          if [ -z "$FILTER" ]; then
            FILTER="."
          fi
          
          PACKAGES=$(go list ./... 2>/dev/null | grep -v -E '/(api/gen|docs/generated)/' || echo "")
          
          if [ -n "$PACKAGES" ]; then
            go test -bench="${FILTER}" -benchmem -count=${BENCHMARK_COUNT} -timeout=30m \
              $PACKAGES 2>&1 | tee _output/benchmark/base.txt || true
          else
            echo "No packages to benchmark" > _output/benchmark/base.txt
          fi

      - name: Compare benchmarks
        id: compare
        run: |
          mkdir -p _output/benchmark
          
          if [ -f "_output/benchmark/base.txt" ] && [ -f "_output/benchmark/current.txt" ]; then
            echo "ðŸ“Š Comparing benchmark results..."
            benchstat _output/benchmark/base.txt _output/benchmark/current.txt > _output/benchmark/comparison.txt 2>&1 || true
            
            echo "### Benchmark Comparison" > _output/benchmark/summary.md
            echo "" >> _output/benchmark/summary.md
            echo '```' >> _output/benchmark/summary.md
            cat _output/benchmark/comparison.txt >> _output/benchmark/summary.md
            echo '```' >> _output/benchmark/summary.md
            
            # Check for significant regressions
            REGRESSIONS=$(grep -E '\+[0-9]+\.[0-9]+%' _output/benchmark/comparison.txt | wc -l || echo "0")
            echo "regressions_found=${REGRESSIONS}" >> $GITHUB_OUTPUT
            
            if [ "$REGRESSIONS" -gt 0 ]; then
              echo "âš ï¸ Found ${REGRESSIONS} potential performance regressions"
              echo "has_regressions=true" >> $GITHUB_OUTPUT
            else
              echo "âœ… No significant performance regressions detected"
              echo "has_regressions=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "âš ï¸ Could not compare benchmarks - missing baseline or current results"
            echo "has_regressions=false" >> $GITHUB_OUTPUT
          fi

      - name: Comment PR with benchmark results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let body = '## ðŸ“Š Performance Benchmark Results\n\n';
            
            const hasRegressions = '${{ steps.compare.outputs.has_regressions }}' === 'true';
            const regressionsFound = '${{ steps.compare.outputs.regressions_found }}';
            
            if (hasRegressions) {
              body += `âš ï¸ **Warning**: Found ${regressionsFound} potential performance regressions.\n\n`;
              body += 'Please review the benchmark comparison below and ensure any regressions are intentional.\n\n';
            } else {
              body += 'âœ… **No significant performance regressions detected.**\n\n';
            }
            
            // Try to read the summary file
            try {
              const summary = fs.readFileSync('_output/benchmark/summary.md', 'utf8');
              body += summary;
            } catch (e) {
              body += 'Benchmark comparison not available.\n';
            }
            
            body += '\n---\n';
            body += `*Benchmark comparison generated by GitHub Actions*\n`;
            body += `*Threshold for regression detection: ${process.env.REGRESSION_THRESHOLD}%*`;
            
            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
            });
            
            const existingComment = comments.find(comment => 
              comment.user.type === 'Bot' && 
              comment.body.includes('Performance Benchmark Results')
            );
            
            if (existingComment) {
              await github.rest.issues.updateComment({
                comment_id: existingComment.id,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: body
              });
            }

      - name: Upload comparison results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-comparison-${{ github.sha }}
          path: _output/benchmark/
          retention-days: 30

  # ===========================================================================
  # Store Baseline (for master branch)
  # ===========================================================================
  store-baseline:
    name: Store Baseline
    runs-on: ubuntu-latest
    needs: benchmark
    if: github.event_name == 'push' && github.ref == 'refs/heads/master'
    steps:
      - name: Download benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: _output/benchmark/

      - name: Upload as baseline
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-baseline-master
          path: _output/benchmark/
          retention-days: 90

      - name: Baseline summary
        run: |
          echo "## ðŸ“Š Benchmark Baseline Updated" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "New baseline stored for master branch." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Commit**: ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "**Date**: $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY

  # ===========================================================================
  # Benchmark Summary
  # ===========================================================================
  summary:
    name: Benchmark Summary
    runs-on: ubuntu-latest
    needs: [benchmark]
    if: always()
    steps:
      - name: Download benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: _output/benchmark/
        continue-on-error: true

      - name: Generate summary
        run: |
          echo "## ðŸƒ Performance Benchmark Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "_output/benchmark/current.txt" ]; then
            echo "### Benchmark Results" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            # Extract key metrics
            echo '```' >> $GITHUB_STEP_SUMMARY
            grep -E "^Benchmark" _output/benchmark/current.txt | head -20 >> $GITHUB_STEP_SUMMARY || echo "No benchmark results found" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            
            # Count benchmarks
            BENCH_COUNT=$(grep -c "^Benchmark" _output/benchmark/current.txt 2>/dev/null || echo "0")
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Total benchmarks run**: ${BENCH_COUNT}" >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ Benchmark results not available" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "*Benchmark run completed at $(date -u '+%Y-%m-%d %H:%M:%S UTC')*" >> $GITHUB_STEP_SUMMARY

